{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e5691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/task_runtime/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Will prompt for token\n",
    "# login()\n",
    "\n",
    "# Option 2: Use token directly (replace with your token)\n",
    "# login(token=\"YOUR_HF_TOKEN_HERE\")\n",
    "\n",
    "# Option 3: Read from environment variable\n",
    "# import os\n",
    "# login(token=os.environ.get(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd4c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-Math-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393324f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      4\u001b[39m messages = [\n\u001b[32m      5\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWho are you?\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      7\u001b[39m inputs = tokenizer.apply_chat_template(\n\u001b[32m      8\u001b[39m \tmessages,\n\u001b[32m      9\u001b[39m \tadd_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \treturn_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m ).to(model.device)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]:]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/myenv/lib/python3.12/site-packages/transformers/generation/utils.py:2543\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2532\u001b[39m     warnings.warn(\n\u001b[32m   2533\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are calling .generate() with the `input_ids` being on a device type different\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2534\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m than your model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms device. `input_ids` is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids.device.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, whereas the model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2539\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   2540\u001b[39m     )\n\u001b[32m   2542\u001b[39m \u001b[38;5;66;03m# 8. prepare logits processors and stopping criteria\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2543\u001b[39m prepared_logits_processor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_logits_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2553\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2554\u001b[39m prepared_stopping_criteria = \u001b[38;5;28mself\u001b[39m._get_stopping_criteria(\n\u001b[32m   2555\u001b[39m     generation_config=generation_config,\n\u001b[32m   2556\u001b[39m     stopping_criteria=stopping_criteria,\n\u001b[32m   2557\u001b[39m     tokenizer=generation_mode_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2560\u001b[39m \u001b[38;5;66;03m# Set model_kwargs `use_cache` so we can use it later in forward runs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/myenv/lib/python3.12/site-packages/transformers/generation/utils.py:1267\u001b[39m, in \u001b[36mGenerationMixin._get_logits_processor\u001b[39m\u001b[34m(self, generation_config, input_ids_seq_length, encoder_input_ids, prefix_allowed_tokens_fn, logits_processor, device, model_kwargs, negative_prompt_ids, negative_prompt_attention_mask)\u001b[39m\n\u001b[32m   1264\u001b[39m \u001b[38;5;66;03m# the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files\u001b[39;00m\n\u001b[32m   1265\u001b[39m \u001b[38;5;66;03m# all samplers can be found in `generation_utils_samplers.py`\u001b[39;00m\n\u001b[32m   1266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m generation_config.temperature != \u001b[32m1.0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m     processors.append(\u001b[43mTemperatureLogitsWarper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.top_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m generation_config.top_k != \u001b[32m0\u001b[39m:\n\u001b[32m   1269\u001b[39m     processors.append(\n\u001b[32m   1270\u001b[39m         TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep)\n\u001b[32m   1271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/myenv/lib/python3.12/site-packages/transformers/generation/logits_process.py:287\u001b[39m, in \u001b[36mTemperatureLogitsWarper.__init__\u001b[39m\u001b[34m(self, temperature)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(temperature, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m temperature == \u001b[32m0.0\u001b[39m:\n\u001b[32m    286\u001b[39m         except_msg += \u001b[33m\"\u001b[39m\u001b[33m If you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre looking for greedy decoding strategies, set `do_sample=False`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(except_msg)\n\u001b[32m    289\u001b[39m \u001b[38;5;28mself\u001b[39m.temperature = temperature\n",
      "\u001b[31mValueError\u001b[39m: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-Math-7B-Instruct\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0654231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae51b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"qwedsacf/competition_math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3247e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.98it/s]\n",
      "The module name qwen2_hyphen_math_hyphen_7b_hyphen_instruct_finetuned_on_first_3542_transformed_omni_math_solutions_filtered_lr:2e_hyphen_05_warmup_steps:300_num_epochs:5 (originally qwen2-math-7b-instruct_finetuned_on_first_3542_transformed_omni_math_solutions_filtered_lr:2e-05_warmup_steps:300_num_epochs:5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151643)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#finetuned_tokenizer = AutoTokenizer.from_pretrained(\"./qwen_finetuned\")\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\"./models/qwen2-math-7b-instruct_finetuned_on_first_3542_transformed_omni_math_solutions_filtered_lr:2e-05_warmup_steps:300_num_epochs:5\")\n",
    "finetuned_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "322de19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12500/12500 [00:00<00:00, 258727.50 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "         151645,    198, 151644,    872,    198,     50,   3948,    419,  17047,\n",
       "           3491,   2041,   1667,    894,   9250,   7375,     13,  10224,    697,\n",
       "           6291,    304,   1124,  79075,  42710,  47402,   3561,    382,   5692,\n",
       "            374,    279,   3491,   1447,  11411,    400,     32,   4080,     19,\n",
       "             11,     16,    701,    425,   4080,     16,     11,     19,  15087,\n",
       "            323,    400,     34,   4080,     16,     11,     16,  15087,    525,\n",
       "            279,  17228,    315,  57960,  55114,  19360,  12947,   3555,    686,\n",
       "            387,    279,  13934,    315,    279,   2168,    315,   1459,    362,\n",
       "            421,  57960,  55114,  19360,      3,    374,  45620,    220,     24,\n",
       "             15,  12348,  65670,    911,    279,   6238,     30, 151645,    198,\n",
       "         151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a geometry level 5 problem from the dataset\n",
    "geometry_level5 = ds['train'].filter(lambda x: x.get('type') == 'Geometry' and x.get('level') == 'Level 4')\n",
    "problem = geometry_level5[2]\n",
    "\n",
    "\n",
    "# Format the problem for the model\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"Solve this geometry problem without using any external tools. Put your solution in \\\\boxed{...} format.\\n\\n Here is the problem:\\n\\n{problem['problem']}\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e93958",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m outputs = \u001b[43mfinetuned_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m response = tokenizer.decode(outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProblem:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/myenv/lib/python3.12/site-packages/transformers/generation/utils.py:2543\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2532\u001b[39m     warnings.warn(\n\u001b[32m   2533\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are calling .generate() with the `input_ids` being on a device type different\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2534\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m than your model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms device. `input_ids` is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids.device.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, whereas the model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2539\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   2540\u001b[39m     )\n\u001b[32m   2542\u001b[39m \u001b[38;5;66;03m# 8. prepare logits processors and stopping criteria\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2543\u001b[39m prepared_logits_processor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_logits_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2553\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2554\u001b[39m prepared_stopping_criteria = \u001b[38;5;28mself\u001b[39m._get_stopping_criteria(\n\u001b[32m   2555\u001b[39m     generation_config=generation_config,\n\u001b[32m   2556\u001b[39m     stopping_criteria=stopping_criteria,\n\u001b[32m   2557\u001b[39m     tokenizer=generation_mode_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2560\u001b[39m \u001b[38;5;66;03m# Set model_kwargs `use_cache` so we can use it later in forward runs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/myenv/lib/python3.12/site-packages/transformers/generation/utils.py:1267\u001b[39m, in \u001b[36mGenerationMixin._get_logits_processor\u001b[39m\u001b[34m(self, generation_config, input_ids_seq_length, encoder_input_ids, prefix_allowed_tokens_fn, logits_processor, device, model_kwargs, negative_prompt_ids, negative_prompt_attention_mask)\u001b[39m\n\u001b[32m   1264\u001b[39m \u001b[38;5;66;03m# the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files\u001b[39;00m\n\u001b[32m   1265\u001b[39m \u001b[38;5;66;03m# all samplers can be found in `generation_utils_samplers.py`\u001b[39;00m\n\u001b[32m   1266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m generation_config.temperature != \u001b[32m1.0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m     processors.append(\u001b[43mTemperatureLogitsWarper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.top_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m generation_config.top_k != \u001b[32m0\u001b[39m:\n\u001b[32m   1269\u001b[39m     processors.append(\n\u001b[32m   1270\u001b[39m         TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep)\n\u001b[32m   1271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/task_runtime/myenv/lib/python3.12/site-packages/transformers/generation/logits_process.py:287\u001b[39m, in \u001b[36mTemperatureLogitsWarper.__init__\u001b[39m\u001b[34m(self, temperature)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(temperature, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m temperature == \u001b[32m0.0\u001b[39m:\n\u001b[32m    286\u001b[39m         except_msg += \u001b[33m\"\u001b[39m\u001b[33m If you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre looking for greedy decoding strategies, set `do_sample=False`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(except_msg)\n\u001b[32m    289\u001b[39m \u001b[38;5;28mself\u001b[39m.temperature = temperature\n",
      "\u001b[31mValueError\u001b[39m: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`."
     ]
    }
   ],
   "source": [
    "outputs = finetuned_model.generate(**inputs, max_new_tokens=512, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Problem:\")\n",
    "print(problem['problem'])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Model Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b67d13ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When we rotate images $90^{\\\\circ}$ the coordinates switch places, and the signs are adjusted based on whether or not an axis was crossed. In this case, rotating point $A$ $90^{\\\\circ}$ will bring it across the $y$-axis into Quadrant I, which means both the $x$ and $y$ will be positive. The original point $A$ was at $(-4, 1)$ so the final image will be at $(1, 4)$. We also could solve this problem by seeing that the slope of the segment from the origin to $A$ is $-1/4$. If $A$ is moving to a location that is a $90^{\\\\circ}$ rotation about the origin, it will move to a point on the segment perpendicular to the one that currently connects it to the origin. This will be the segment that has a slope of 4/1 or $-4/-1$ from the origin which puts us at $(1, 4)$ or $(-1, -4)$. The point $\\\\boxed{(1, 4)}$ is in the clockwise direction we need.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem['solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ab73ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "generated_dataset_level5 = datasets.load_from_disk('data/math_solutions_dataset_20000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aaf1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_dataset_level5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c4e2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset length: 553\n",
      "Dataset features: {'problem': Value('string'), 'ground_truth': Value('string'), 'solution': Value('string')}\n",
      "\n",
      "First example:\n",
      "{'problem': 'Square ABCD has its center at $(8,-8)$ and has an area of 4 square units. The top side of the square is horizontal. The square is then dilated with the dilation center at (0,0) and a scale factor of 2. What are the coordinates of the vertex of the image of square ABCD that is farthest from the origin? Give your answer as an ordered pair.', 'ground_truth': 'With the center of dilation at the origin and a scale factor of 2, all the coordinates of square $ABCD$ are twice the coordinates of its preimage. The preimage has an area of 4 square units, so its side length is 2 units. Since the center of the preimage is at $(8, -8)$, the four vertices of the preimage are at $(7, -9), (7, -7), (9, -7)$ and $(9, -9)$. The point $(9, -9)$ is the farthest from the origin on the preimage, so the point farthest from the origin on the image of square $ABCD$ is $\\\\boxed{(18, -18)}.$', 'solution': \"\\\\begin{intermediatederivation}\\nThe square has area \\\\(4\\\\), so its side length is \\\\(s=\\\\sqrt{4}=2\\\\). The half-side is \\\\(h=\\\\tfrac{s}{2}=1\\\\). With center \\\\(C=(8,-8)\\\\) and the top side horizontal, the square is axis-aligned. Thus\\n\\\\[\\nx_L=8-1=7,\\\\quad x_R=8+1=9,\\\\quad y_T=-8+1=-7,\\\\quad y_B=-8-1=-9.\\n\\\\]\\nThe four vertices are\\n\\\\[\\n(7,-7),\\\\quad (9,-7),\\\\quad (7,-9),\\\\quad (9,-9).\\n\\\\]\\nTheir squared distances from the origin are\\n\\\\[\\n98,\\\\quad 130,\\\\quad 130,\\\\quad 162,\\n\\\\]\\nrespectively, so the farthest original vertex from the origin is \\\\(V=(9,-9)\\\\).\\n\\\\end{intermediatederivation}\\n\\n\\\\begin{lemmatheorem}\\n\\\\textbf{Statement:} A dilation with center at the origin and scale factor \\\\(k\\\\) maps any point \\\\((x,y)\\\\) to \\\\((kx,ky)\\\\) and multiplies its distance from the origin by \\\\(|k|\\\\).\\n\\\\textbf{Name:} Dilation (geometry)\\n\\\\textbf{Wikipedia URL:} https://en.wikipedia.org/wiki/Dilation_(geometry)\\n\\\\textbf{Topic:} geometry\\n\\\\end{lemmatheorem}\\n\\n\\\\begin{intermediatederivation}\\nApplying the dilation with scale factor \\\\(k=2\\\\) to \\\\(V=(9,-9)\\\\) gives\\n\\\\[\\nV'=(2\\\\cdot 9,\\\\,2\\\\cdot (-9))=(18,-18).\\n\\\\]\\nSince distances from the origin scale by \\\\(2\\\\), \\\\(V'\\\\) is the farthest image vertex from the origin.\\n\\\\end{intermediatederivation}\\n\\n\\\\[\\\\boxed{(18,-18)}\\\\]\"}\n"
     ]
    }
   ],
   "source": [
    "# Reload the dataset to ensure it's loaded correctly\n",
    "from datasets import load_from_disk\n",
    "\n",
    "generated_dataset_level5 = load_from_disk('data/math_solutions_dataset_20000/')\n",
    "\n",
    "# Check the dataset structure\n",
    "print(f\"Dataset type: {type(generated_dataset_level5)}\")\n",
    "print(f\"Dataset length: {len(generated_dataset_level5)}\")\n",
    "if hasattr(generated_dataset_level5, 'features'):\n",
    "    print(f\"Dataset features: {generated_dataset_level5.features}\")\n",
    "    print(f\"\\nFirst example:\")\n",
    "    print(generated_dataset_level5[0])\n",
    "else:\n",
    "    print(f\"Error: Dataset is not a proper Dataset object. It's a {type(generated_dataset_level5)}\")\n",
    "    print(f\"First item: {generated_dataset_level5[0] if len(generated_dataset_level5) > 0 else 'empty'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Square ABCD has its center at $(8,-8)$ and has an area of 4 square units. The top side of the square is horizontal. The square is then dilated with the dilation center at (0,0) and a scale factor of 2. What are the coordinates of the vertex of the image of square ABCD that is farthest from the origin? Give your answer as an ordered pair.',\n",
       " 'ground_truth': 'With the center of dilation at the origin and a scale factor of 2, all the coordinates of square $ABCD$ are twice the coordinates of its preimage. The preimage has an area of 4 square units, so its side length is 2 units. Since the center of the preimage is at $(8, -8)$, the four vertices of the preimage are at $(7, -9), (7, -7), (9, -7)$ and $(9, -9)$. The point $(9, -9)$ is the farthest from the origin on the preimage, so the point farthest from the origin on the image of square $ABCD$ is $\\\\boxed{(18, -18)}.$',\n",
       " 'solution': \"\\\\begin{intermediatederivation}\\nThe square has area \\\\(4\\\\), so its side length is \\\\(s=\\\\sqrt{4}=2\\\\). The half-side is \\\\(h=\\\\tfrac{s}{2}=1\\\\). With center \\\\(C=(8,-8)\\\\) and the top side horizontal, the square is axis-aligned. Thus\\n\\\\[\\nx_L=8-1=7,\\\\quad x_R=8+1=9,\\\\quad y_T=-8+1=-7,\\\\quad y_B=-8-1=-9.\\n\\\\]\\nThe four vertices are\\n\\\\[\\n(7,-7),\\\\quad (9,-7),\\\\quad (7,-9),\\\\quad (9,-9).\\n\\\\]\\nTheir squared distances from the origin are\\n\\\\[\\n98,\\\\quad 130,\\\\quad 130,\\\\quad 162,\\n\\\\]\\nrespectively, so the farthest original vertex from the origin is \\\\(V=(9,-9)\\\\).\\n\\\\end{intermediatederivation}\\n\\n\\\\begin{lemmatheorem}\\n\\\\textbf{Statement:} A dilation with center at the origin and scale factor \\\\(k\\\\) maps any point \\\\((x,y)\\\\) to \\\\((kx,ky)\\\\) and multiplies its distance from the origin by \\\\(|k|\\\\).\\n\\\\textbf{Name:} Dilation (geometry)\\n\\\\textbf{Wikipedia URL:} https://en.wikipedia.org/wiki/Dilation_(geometry)\\n\\\\textbf{Topic:} geometry\\n\\\\end{lemmatheorem}\\n\\n\\\\begin{intermediatederivation}\\nApplying the dilation with scale factor \\\\(k=2\\\\) to \\\\(V=(9,-9)\\\\) gives\\n\\\\[\\nV'=(2\\\\cdot 9,\\\\,2\\\\cdot (-9))=(18,-18).\\n\\\\]\\nSince distances from the origin scale by \\\\(2\\\\), \\\\(V'\\\\) is the farthest image vertex from the origin.\\n\\\\end{intermediatederivation}\\n\\n\\\\[\\\\boxed{(18,-18)}\\\\]\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_dataset_level5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5e971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "filtered_dataset = datasets.load_from_disk('newopenaioutputs/transformed_solutions_qwen2-math-7b-instruct_filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a65d1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd12710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
